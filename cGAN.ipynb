{"cells":[{"metadata":{"_uuid":"4b9702ed-2ccf-4aea-8c94-1f83ce9d33c6","_cell_guid":"f6907662-d986-4577-9a96-3367a300d46b","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import *\nfrom keras.layers.merge import concatenate\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import load_model\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport os\nimport argparse","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DFtrain = pd.read_csv('../input/computer-vision-learning-contest-dacon/train.csv')\ntest = pd.read_csv('../input/computer-vision-learning-contest-dacon/test.csv')\nsubmission = pd.read_csv('../input/computer-vision-learning-contest-dacon/submission.csv')","execution_count":127,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = (DFtrain[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 28, 28, 1)\ny_train = to_categorical(DFtrain['digit'].values)","execution_count":128,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.randint(0, 2048)\nimg = DFtrain.loc[idx, '0':].values.reshape(28, 28).astype(int)\ndigit = DFtrain.loc[idx, 'digit']\nletter = DFtrain.loc[idx, 'letter']\n\nplt.title('Index: %i, Digit: %s, Letter: %s'%(idx, digit, letter))\nplt.imshow(img)\nplt.show()","execution_count":129,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_generator(inputs, labels, image_size):\n    image_resize = image_size // 4\n    # network parameters\n    kernel_size = 5\n    layer_filters = [128, 64, 32, 1]\n\n    x = concatenate([inputs, labels], axis=1)\n    x = Dense(image_resize * image_resize * layer_filters[0])(x)\n    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n\n    for filters in layer_filters:\n        # first two convolution layers use strides = 2\n        # the last two use strides = 1\n        if filters > layer_filters[-2]:\n            strides = 2\n        else:\n            strides = 1\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv2DTranspose(filters=filters,\n                            kernel_size=kernel_size,\n                            strides=strides,\n                            padding='same')(x)\n\n    x = Activation('sigmoid')(x)\n    # input is conditioned by labels\n    generator = Model([inputs, labels], x, name='generator')\n    return generator","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_discriminator(inputs, labels, image_size):\n    kernel_size = 5\n    layer_filters = [32, 64, 128, 256]\n\n    x = inputs\n\n    y = Dense(image_size * image_size)(labels)\n    y = Reshape((image_size, image_size, 1))(y)\n    x = concatenate([x, y])\n\n    for filters in layer_filters:\n        # first 3 convolution layers use strides = 2\n        # last one uses strides = 1\n        if filters == layer_filters[-1]:\n            strides = 1\n        else:\n            strides = 2\n        x = LeakyReLU(alpha=0.2)(x)\n        x = Conv2D(filters=filters,\n                   kernel_size=kernel_size,\n                   strides=strides,\n                   padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(1)(x)\n    x = Activation('sigmoid')(x)\n    # input is conditioned by labels\n    discriminator = Model([inputs, labels], x, name='discriminator')\n    return discriminator\n\n","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(models, data, params):\n    # the GAN models\n    generator, discriminator, adversarial = models\n    # network parameters\n    batch_size, latent_size, train_steps, num_labels, model_name = params\n    # the generator image is saved every 500 steps\n    save_interval = 500\n    # noise vector to see how the generator output evolves during training\n    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n    # one-hot label the noise will be conditioned to\n    noise_class = np.eye(num_labels)[np.arange(0, 16) % num_labels]\n    # number of elements in train dataset\n    train_size = x_train.shape[0]\n\n    print(model_name,\n          \"Labels for generated images: \",\n          np.argmax(noise_class, axis=1))\n\n    for i in range(train_steps):\n        # train the discriminator for 1 batch\n        # 1 batch of real (label=1.0) and fake images (label=0.0)\n        # randomly pick real images from dataset\n        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n        real_images = x_train[rand_indexes]\n        # corresponding one-hot labels of real images\n        real_labels = y_train[rand_indexes]\n        # generate fake images from noise using generator\n        # generate noise using uniform distribution\n        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n        # assign random one-hot labels\n        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n                                                          batch_size)]\n\n        # generate fake images conditioned on fake labels\n        fake_images = generator.predict([noise, fake_labels])\n        # real + fake images = 1 batch of train data\n        x = np.concatenate((real_images, fake_images))\n        # real + fake one-hot labels = 1 batch of train one-hot labels\n        labels = np.concatenate((real_labels, fake_labels))\n\n        # label real and fake images\n        # real images label is 1.0\n        y = np.ones([2 * batch_size, 1])\n        # fake images label is 0.0\n        y[batch_size:, :] = 0.0\n        # train discriminator network, log the loss and accuracy\n        loss, acc = discriminator.train_on_batch([x, labels], y)\n        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n\n        # train the adversarial network for 1 batch\n        # 1 batch of fake images conditioned on fake 1-hot labels w/ label=1.0\n        # since the discriminator weights are frozen in adversarial network\n        # only the generator is trained\n        # generate noise using uniform distribution        \n        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n        # assign random one-hot labels\n        fake_labels = np.eye(num_labels)[np.random.choice(num_labels,\n                                                           batch_size)]\n        # label fake images as real or 1.0\n        y = np.ones([batch_size, 1])\n        # train the adversarial network \n        # note that unlike in discriminator training, \n        # we do not save the fake images in a variable\n        # the fake images go to the discriminator input of the adversarial\n        # for classification\n        # log the loss and accuracy\n        loss, acc = adversarial.train_on_batch([noise, fake_labels], y)\n        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n        print(log)\n        if (i + 1) % save_interval == 0:\n            if (i + 1) == train_steps:\n                show = True\n            else:\n                show = False\n\n            # plot generator images on a periodic basis\n            plot_images(generator,\n                        noise_input=noise_input,\n                        noise_class=noise_class,\n                        show=show,\n                        step=(i + 1),\n                        model_name=model_name)\n    \n    # save the model after training the generator\n    # the trained generator can be reloaded for future MNIST digit generation\n    generator.save(model_name + \".h5\")\n\n","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images(generator,\n                noise_input,\n                noise_class,\n                show=False,\n                step=0,\n                model_name=\"gan\"):\n    \"\"\"Generate fake images and plot them\n    For visualization purposes, generate fake images\n    then plot them in a square grid\n    # Arguments\n        generator (Model): The Generator Model for fake images generation\n        noise_input (ndarray): Array of z-vectors\n        show (bool): Whether to show plot or not\n        step (int): Appended to filename of the save images\n        model_name (string): Model name\n    \"\"\"\n    os.makedirs(model_name, exist_ok=True)\n    filename = os.path.join(model_name, \"%05d.png\" % step)\n    images = generator.predict([noise_input, noise_class])\n    print(model_name , \" labels for generated images: \", np.argmax(noise_class, axis=1))\n    plt.figure(figsize=(2.2, 2.2))\n    num_images = images.shape[0]\n    image_size = images.shape[1]\n    rows = int(math.sqrt(noise_input.shape[0]))\n    for i in range(num_images):\n        plt.subplot(rows, rows, i + 1)\n        image = np.reshape(images[i], [image_size, image_size])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.savefig(filename)\n    if show:\n        plt.show()\n    else:\n        plt.close('all')","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_and_train_models():\n    # reshape data for CNN as (28, 28, 1) and normalize\n    x_train = (DFtrain[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 28, 28, 1)\n    y_train = to_categorical(DFtrain['digit'].values)\n    \n    image_size = x_train.shape[1]\n    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n    x_train = x_train.astype('float32') / 255\n\n    num_labels = np.amax(y_train) + 1\n    y_train = to_categorical(y_train)\n\n    model_name = \"cgan_num\"\n    # network parameters\n    # the latent or z vector is 100-dim\n    latent_size = 100\n    batch_size = 64\n    train_steps = 40000\n    lr = 2e-4\n    decay = 6e-8\n    input_shape = (image_size, image_size, 1)\n    label_shape = (num_labels, )\n\n    # build discriminator model\n    inputs = Input(shape=input_shape, name='discriminator_input')\n#     labels = Input(shape=label_shape, name='class_labels')\n    labels = x_train.shape\n\n    discriminator = build_discriminator(inputs, labels, image_size)\n    # [1] or original paper uses Adam, \n    # but discriminator converges easily with RMSprop\n    optimizer = RMSprop(lr=lr, decay=decay)\n    discriminator.compile(loss='binary_crossentropy',\n                          optimizer=optimizer,\n                          metrics=['accuracy'])\n    discriminator.summary()\n\n    # build generator model\n    input_shape = (latent_size, )\n    inputs = Input(shape=input_shape, name='z_input')\n    generator = build_generator(inputs, labels, image_size)\n    generator.summary()\n\n    # build adversarial model = generator + discriminator\n    optimizer = RMSprop(lr=lr*0.5, decay=decay*0.5)\n    # freeze the weights of discriminator during adversarial training\n    discriminator.trainable = False\n    outputs = discriminator([generator([inputs, labels]), labels])\n    adversarial = Model([inputs, labels],\n                        outputs,\n                        name=model_name)\n    adversarial.compile(loss='binary_crossentropy',\n                        optimizer=optimizer,\n                        metrics=['accuracy'])\n    adversarial.summary()\n\n    # train discriminator and adversarial networks\n    models = (generator, discriminator, adversarial)\n    data = (x_train, y_train)\n    params = (batch_size, latent_size, train_steps, num_labels, model_name)\n    train(models, data, params)","execution_count":134,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_generator(generator, class_label=None):\n    noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n    step = 0\n    if class_label is None:\n        num_labels = 10\n        noise_class = np.eye(num_labels)[np.random.choice(num_labels, 16)]\n    else:\n        noise_class = np.zeros((16, 10))\n        noise_class[:,class_label] = 1\n        step = class_label\n\n    plot_images(generator,\n                noise_input=noise_input,\n                noise_class=noise_class,\n                show=True,\n                step=step,\n                model_name=\"test_outputs\")","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parser = argparse.ArgumentParser()\n# help_ = \"Load generator h5 model with trained weights\"\n# parser.add_argument(\"-g\", \"--generator\", help=help_)\n# help_ = \"Specify a specific digit to generate\"\n# parser.add_argument(\"-d\", \"--digit\", type=int, help=help_)\n# args = parser.parse_args()\n\n# if args.generator:\n#     generator = load_model(args.generator)\n#     class_label = None\n#     if args.digit is not None:\n#         class_label = args.digit\n#     test_generator(generator, class_label)\n# else:\n#     build_and_train_models()\n\nbuild_and_train_models()","execution_count":136,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Layer dense_1 expects 1 inputs, but it received 4 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=2048>, <tf.Tensor: shape=(), dtype=int32, numpy=28>, <tf.Tensor: shape=(), dtype=int32, numpy=28>, <tf.Tensor: shape=(), dtype=int32, numpy=1>]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-136-8cce7c21684b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     build_and_train_models()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mbuild_and_train_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-134-25c10c45e613>\u001b[0m in \u001b[0;36mbuild_and_train_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# [1] or original paper uses Adam,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# but discriminator converges easily with RMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-131-62e5b77a4e7e>\u001b[0m in \u001b[0;36mbuild_discriminator\u001b[0;34m(inputs, labels, image_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2618\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2619\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' inputs, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                      \u001b[0;34m'but it received '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                      ' input tensors. Inputs received: ' + str(inputs))\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer dense_1 expects 1 inputs, but it received 4 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=2048>, <tf.Tensor: shape=(), dtype=int32, numpy=28>, <tf.Tensor: shape=(), dtype=int32, numpy=28>, <tf.Tensor: shape=(), dtype=int32, numpy=1>]"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}